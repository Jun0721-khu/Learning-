{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled15.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq_k5X0RRBv1"
      },
      "source": [
        "Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKOudPpRBJH",
        "outputId": "870ba162-f634-4440-9f58-69728d11174f"
      },
      "source": [
        "from __future__ import print_function\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "POLY_DEGREE = 4\n",
        "W_target = torch.randn(POLY_DEGREE, 1)*5 # for Range setting\n",
        "b_target = torch.randn(1)*5\n",
        "\n",
        "def make_features(x):\n",
        "  \"\"\"Builds features a matrix with columns [x, x^2, x^3, x^4]\"\"\"\n",
        "  # squeeze : 1인 차원을 제거하는 함수\n",
        "  # unsqueeze : 1인 차원을 생성하는 함수ef make_features(x):\n",
        "\n",
        "  x = x.unsqueeze(1)\n",
        "\n",
        "  return torch.cat([x**i for i in range(1, POLY_DEGREE+1)], 1)\n",
        "\n",
        "def f(x):\n",
        "  \"\"\"Approximated function\"\"\"\n",
        "  # mm : matrix multiplication \n",
        "  # x.mm(w) = x1w1+x2w2+...+xnwn\n",
        "  return x.mm(W_target) + b_target.item()\n",
        "\n",
        "def poly_desc(W, b):\n",
        "  \"\"\"Creates a string description of a polynomial\"\"\"\n",
        "  result = 'y = '\n",
        "  for i, w in enumerate(W):\n",
        "    result += '{:+.2f} x^{}'.format(w, i+1)\n",
        "  result += '{:+.2f}'.format(b[0])\n",
        "  return result\n",
        "\n",
        "def get_batch(batch_size = 32):\n",
        "  \"\"\"Builds a batch i. e. (x, f(x)) pair\"\"\"\n",
        "  random = torch.randn(batch_size)\n",
        "  x = make_features(random)\n",
        "  y = f(x)\n",
        "  return x, y\n",
        "\n",
        "#Define Model\n",
        "fc = torch.nn.Linear(W_target.size(0), 1)\n",
        "#count(1) -> 1, 2, 3, 4 ...\n",
        "for batch_idx in count(1):\n",
        "  #Get data\n",
        "  batch_x, batch_y = get_batch()\n",
        "  #Reset Gradients\n",
        "  fc.zero_grad()\n",
        "  #Forward Pass\n",
        "  output = F.smooth_l1_loss(fc(batch_x), batch_y)#L1과 L2 norm의 조합으로 만들어진 Error 함수\n",
        "  # regurarization을 위해 사용함\n",
        "  loss = output.item()\n",
        "  #Backward pass\n",
        "  output.backward()\n",
        "\n",
        "  #Apply gradient\n",
        "  #fc.parameters : Linear에 사용된 parameters  -> w1~w4를 불러옴\n",
        "  #param.grad : fc.parameters의 gradient를 불러옴\n",
        "  #param.data.add(-0.1*param.grad) -> 0.1의 학습률로 grad 업데이트\n",
        "  for param in fc.parameters():\n",
        "    param.data.add_(-0.1*param.grad)  \n",
        "  #Stop criterion\n",
        "\n",
        "  if loss <1e-3:\n",
        "    break\n",
        "\n",
        "print('Loss : {:.6f} after {} batched'.format(loss, batch_idx))\n",
        "print('==> Learned function : \\t'+poly_desc(fc.weight.view(-1), fc.bias))\n",
        "print('==> Actual function : \\t' + poly_desc(W_target.view(-1), b_target))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss : 0.000714 after 685 batched\n",
            "==> Learned function : \ty = +2.85 x^1-11.08 x^2-0.44 x^3-2.95 x^4-1.23\n",
            "==> Actual function : \ty = +2.89 x^1-10.98 x^2-0.47 x^3-2.99 x^4-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGFmVdZRVQw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}